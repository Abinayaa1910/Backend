{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026395ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel('sample_customer_database_5000_singapore.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621ab285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Customer ID        Full Name                Email Address  Phone Number  \\\n",
      "0       C0001     Norma Fisher          ysullivan@yahoo.com      82421948   \n",
      "1       C0002      Levi Durham            qgrimes@gmail.com      97535139   \n",
      "2       C0003   Kimberly Olsen  sean96@johnston-roberts.com      71122018   \n",
      "3       C0004   Matthew Davies    nguyendarrell@hotmail.com      41352560   \n",
      "4       C0005  Angela Martinez    myersmitchell@johnson.com        869141   \n",
      "\n",
      "  Date Joined     Location  Gender Loyalty Tier  \\\n",
      "0  2023-08-11     Tampines  Female     Platinum   \n",
      "1  2022-11-24      Geylang  Female     Platinum   \n",
      "2  2023-06-19     Tampines  Female     Platinum   \n",
      "3  2025-04-04   Ang Mo Kio    Male       Silver   \n",
      "4  2025-01-15  Bukit Batok  Female     Platinum   \n",
      "\n",
      "                                               Notes  \n",
      "0                        Together range line beyond.  \n",
      "1  Language ball floor meet usually board necessary.  \n",
      "2                 Support time operation wear often.  \n",
      "3                                  Stage plant view.  \n",
      "4          Job article level others record hospital.  \n"
     ]
    }
   ],
   "source": [
    "# First look at the data\n",
    "print(df.head())  # First 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ae35ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "\n",
    "print(\"NumPy version:\", numpy.__version__)\n",
    "print(\"Pandas version:\", pandas.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ccb804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (5000, 9)\n"
     ]
    }
   ],
   "source": [
    "# Shape of the dataset\n",
    "print(\"Shape of dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbeab334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Customer ID    5000 non-null   object        \n",
      " 1   Full Name      5000 non-null   object        \n",
      " 2   Email Address  5000 non-null   object        \n",
      " 3   Phone Number   5000 non-null   int64         \n",
      " 4   Date Joined    5000 non-null   datetime64[ns]\n",
      " 5   Location       5000 non-null   object        \n",
      " 6   Gender         5000 non-null   object        \n",
      " 7   Loyalty Tier   5000 non-null   object        \n",
      " 8   Notes          5000 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(7)\n",
      "memory usage: 351.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Columns and Data types\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2290bca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " Customer ID      0\n",
      "Full Name        0\n",
      "Email Address    0\n",
      "Phone Number     0\n",
      "Date Joined      0\n",
      "Location         0\n",
      "Gender           0\n",
      "Loyalty Tier     0\n",
      "Notes            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16c175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"Number of duplicate rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b711366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values per column:\n",
      " Customer ID      5000\n",
      "Full Name        4835\n",
      "Email Address    4983\n",
      "Phone Number     4998\n",
      "Date Joined      1084\n",
      "Location           27\n",
      "Gender              2\n",
      "Loyalty Tier        3\n",
      "Notes            5000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Unique values per column\n",
    "print(\"Unique values per column:\\n\", df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dcac679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', 'sentence', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "tokens = word_tokenize(\"This is a test sentence.\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e6b7a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20672a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Notes  \\\n",
      "0                        Together range line beyond.   \n",
      "1  Language ball floor meet usually board necessary.   \n",
      "2                 Support time operation wear often.   \n",
      "3                                  Stage plant view.   \n",
      "4          Job article level others record hospital.   \n",
      "\n",
      "                                      Cleaned_Notes  \n",
      "0                        together range line beyond  \n",
      "1  language ball floor meet usually board necessary  \n",
      "2                 support time operation wear often  \n",
      "3                                  stage plant view  \n",
      "4          job article level others record hospital  \n"
     ]
    }
   ],
   "source": [
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Join back into string\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply to Notes column\n",
    "df['Cleaned_Notes'] = df['Notes'].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Preview\n",
    "print(df[['Notes', 'Cleaned_Notes']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cfdb564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (5000, 869)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the cleaned notes\n",
    "tfidf_matrix = tfidf.fit_transform(df['Cleaned_Notes'])\n",
    "\n",
    "# Check shape (rows = customers, columns = tokens)\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa6cc293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ability  able  accept  according  account  across  act  action  activity  \\\n",
      "0      0.0   0.0     0.0        0.0      0.0     0.0  0.0     0.0       0.0   \n",
      "1      0.0   0.0     0.0        0.0      0.0     0.0  0.0     0.0       0.0   \n",
      "2      0.0   0.0     0.0        0.0      0.0     0.0  0.0     0.0       0.0   \n",
      "3      0.0   0.0     0.0        0.0      0.0     0.0  0.0     0.0       0.0   \n",
      "4      0.0   0.0     0.0        0.0      0.0     0.0  0.0     0.0       0.0   \n",
      "\n",
      "   actually  ...  would  write  writer  wrong  yard  yeah  year  yes  yet  \\\n",
      "0       0.0  ...    0.0    0.0     0.0    0.0   0.0   0.0   0.0  0.0  0.0   \n",
      "1       0.0  ...    0.0    0.0     0.0    0.0   0.0   0.0   0.0  0.0  0.0   \n",
      "2       0.0  ...    0.0    0.0     0.0    0.0   0.0   0.0   0.0  0.0  0.0   \n",
      "3       0.0  ...    0.0    0.0     0.0    0.0   0.0   0.0   0.0  0.0  0.0   \n",
      "4       0.0  ...    0.0    0.0     0.0    0.0   0.0   0.0   0.0  0.0  0.0   \n",
      "\n",
      "   young  \n",
      "0    0.0  \n",
      "1    0.0  \n",
      "2    0.0  \n",
      "3    0.0  \n",
      "4    0.0  \n",
      "\n",
      "[5 rows x 869 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense DataFrame for inspection\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Preview the first few rows\n",
    "print(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6e671b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Cleaned_Notes  Text_Cluster_Label\n",
      "0                        together range line beyond                   0\n",
      "1  language ball floor meet usually board necessary                   3\n",
      "2                 support time operation wear often                   4\n",
      "3                                  stage plant view                   4\n",
      "4          job article level others record hospital                   4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Cleaned_Notes'])\n",
    "\n",
    "# Step 2: Spectral Clustering\n",
    "num_clusters = 5  # Adjust as needed\n",
    "spectral = SpectralClustering(\n",
    "    n_clusters=num_clusters,\n",
    "    affinity='nearest_neighbors',  # or 'rbf' if dense similarity matrix preferred\n",
    "    assign_labels='kmeans',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "labels = spectral.fit_predict(tfidf_matrix.toarray())  # Convert sparse matrix to dense\n",
    "\n",
    "# Step 3: Add cluster labels to DataFrame\n",
    "df['Text_Cluster_Label'] = labels\n",
    "\n",
    "# Step 4: Preview Results\n",
    "print(df[['Cleaned_Notes', 'Text_Cluster_Label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edbacb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Location  Gender Loyalty Tier  Categorical_Cluster_Label\n",
      "0     Tampines  Female     Platinum                          4\n",
      "1      Geylang  Female     Platinum                          4\n",
      "2     Tampines  Female     Platinum                          4\n",
      "3   Ang Mo Kio    Male       Silver                          4\n",
      "4  Bukit Batok  Female     Platinum                          4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\marketing-portal\\.venv\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Select categorical columns\n",
    "categorical_cols = ['Location', 'Gender', 'Loyalty Tier']\n",
    "\n",
    "# Use OneHotEncoder without the sparse argument\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "cat_encoded_sparse = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Convert to dense only if needed\n",
    "cat_encoded = cat_encoded_sparse.toarray() if hasattr(cat_encoded_sparse, \"toarray\") else cat_encoded_sparse\n",
    "\n",
    "# Apply Spectral Clustering\n",
    "spectral = SpectralClustering(\n",
    "    n_clusters=5,\n",
    "    affinity='nearest_neighbors',\n",
    "    assign_labels='kmeans',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "labels = spectral.fit_predict(cat_encoded)\n",
    "\n",
    "# Save labels\n",
    "df['Categorical_Cluster_Label'] = labels\n",
    "\n",
    "# View result\n",
    "print(df[['Location', 'Gender', 'Loyalty Tier', 'Categorical_Cluster_Label']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41caaec5",
   "metadata": {},
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Filter out rows with empty Cleaned_Notes again to be safe\n",
    "df = df[df['Cleaned_Notes'].str.strip() != '']\n",
    "\n",
    "# Refit TF-IDF just in case rows were dropped\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Cleaned_Notes'])\n",
    "\n",
    "spectral = SpectralClustering(\n",
    "    n_clusters=5,\n",
    "    affinity='nearest_neighbors',\n",
    "    assign_labels='kmeans',\n",
    "    random_state=42\n",
    ")\n",
    "labels = spectral.fit_predict(tfidf_matrix.toarray())\n",
    "\n",
    "# Assign cluster labels to the dataframe\n",
    "df['Text_Cluster_Label'] = labels\n",
    "\n",
    "# Preview the clustering output\n",
    "print(df[['Cleaned_Notes', 'Text_Cluster_Label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64d37f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6ee6bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\marketing-portal\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Cleaned_Notes  HDBSCAN_Text_Label\n",
      "0                        together range line beyond                  -1\n",
      "1  language ball floor meet usually board necessary                  -1\n",
      "2                 support time operation wear often                  -1\n",
      "3                                  stage plant view                  -1\n",
      "4          job article level others record hospital                  -1\n",
      "5                              animal exactly drive                   0\n",
      "6               sign remember close ask reduce land                  -1\n",
      "7                                     part cup read                  -1\n",
      "8      republican total policy head mrs debate onto                  -1\n",
      "9           rock structure federal board night loss                  -1\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')\n",
    "\n",
    "# Fit to TF-IDF matrix\n",
    "labels = clusterer.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Add the cluster labels to your dataframe\n",
    "df['HDBSCAN_Text_Label'] = labels\n",
    "# Check results\n",
    "print(df[['Cleaned_Notes', 'HDBSCAN_Text_Label']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5ad8aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\marketing-portal\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Location  Gender Loyalty Tier  HDBSCAN_Categorical_Label\n",
      "0     Tampines  Female     Platinum                         23\n",
      "1      Geylang  Female     Platinum                         24\n",
      "2     Tampines  Female     Platinum                         23\n",
      "3   Ang Mo Kio    Male       Silver                         29\n",
      "4  Bukit Batok  Female     Platinum                         25\n",
      "5     Tampines    Male       Silver                          6\n",
      "6  Bukit Batok  Female       Silver                         26\n",
      "7        Bedok  Female       Silver                          4\n",
      "8    Woodlands  Female       Silver                          5\n",
      "9    Pasir Ris    Male       Silver                         55\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Select the categorical columns\n",
    "categorical_cols = ['Location', 'Gender', 'Loyalty Tier']\n",
    "\n",
    "# Apply One-Hot Encoding (sparse_output=True for HDBSCAN compatibility)\n",
    "encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "categorical_matrix = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "# Initialize and fit HDBSCAN\n",
    "hdbscan_cat = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')\n",
    "cat_labels = hdbscan_cat.fit_predict(categorical_matrix)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "df['HDBSCAN_Categorical_Label'] = cat_labels\n",
    "\n",
    "print(df[['Location', 'Gender', 'Loyalty Tier', 'HDBSCAN_Categorical_Label']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec005ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "946c2ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Location Gender Loyalty Tier                          Cleaned_Notes  \\\n",
      "0      Serangoon   Male       Silver                            outside low   \n",
      "1        Kallang   Male         Gold  morning region industry term director   \n",
      "2  Choa Chu Kang   Male       Silver  degree executive attention argue hold   \n",
      "3     Queenstown   Male         Gold                    standard race least   \n",
      "4   Central Area   Male     Platinum                     put bag seven stay   \n",
      "\n",
      "   Gower_Agglo_Label  \n",
      "0                  1  \n",
      "1                  2  \n",
      "2                  1  \n",
      "3                  2  \n",
      "4                  4  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import gower\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Encode categorical columns\n",
    "categorical_cols = ['Location', 'Gender', 'Loyalty Tier']\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "cat_encoded = encoder.fit_transform(df_sample[categorical_cols])\n",
    "\n",
    "# Step 3: TF-IDF for text\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df_sample['Cleaned_Notes'].astype(str))\n",
    "\n",
    "# Step 4: Reduce TF-IDF dimensions\n",
    "svd = TruncatedSVD(n_components=10, random_state=42)\n",
    "text_reduced = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Step 5: Combine categorical and text features\n",
    "combined_features = np.hstack([cat_encoded, text_reduced])\n",
    "\n",
    "# Step 6: Compute Gower distance\n",
    "gower_dist = gower.gower_matrix(pd.DataFrame(combined_features))\n",
    "\n",
    "# Step 7: Agglomerative clustering\n",
    "agglo = AgglomerativeClustering(n_clusters=5, linkage='average', metric='precomputed')\n",
    "labels = agglo.fit_predict(gower_dist)\n",
    "\n",
    "# Step 8: Add to DataFrame\n",
    "df_sample['Gower_Agglo_Label'] = labels\n",
    "\n",
    "# Step 9: Preview\n",
    "print(df_sample[['Location', 'Gender', 'Loyalty Tier', 'Cleaned_Notes', 'Gower_Agglo_Label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c46f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b869c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a9fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac63975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb239d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06010d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b863491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
