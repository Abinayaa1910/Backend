{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c487ccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded: (5000, 9)\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import umap\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel(\"sample_customer_database_5000_singapore.xlsx\")\n",
    "print(\"✅ Dataset loaded:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12203864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date Joined'] = pd.to_datetime(df['Date Joined'], errors='coerce')\n",
    "\n",
    "# Today's date\n",
    "today = pd.to_datetime(\"today\")\n",
    "\n",
    "# Feature Engineering from Date_Joined\n",
    "df['Days_Since_Joined'] = (today - df['Date Joined']).dt.days\n",
    "df['Join_Year'] = df['Date Joined'].dt.year\n",
    "df['Join_Month'] = df['Date Joined'].dt.month\n",
    "df['Join_Quarter'] = df['Date Joined'].dt.quarter\n",
    "df['Is_New'] = (df['Days_Since_Joined'] <= 180).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13424ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Notes  \\\n",
      "0                        Together range line beyond.   \n",
      "1  Language ball floor meet usually board necessary.   \n",
      "2                 Support time operation wear often.   \n",
      "3                                  Stage plant view.   \n",
      "4          Job article level others record hospital.   \n",
      "\n",
      "                                      Cleaned_Notes  \n",
      "0                        together range line beyond  \n",
      "1  language ball floor meet usually board necessary  \n",
      "2                 support time operation wear often  \n",
      "3                                  stage plant view  \n",
      "4          job article level others record hospital  \n"
     ]
    }
   ],
   "source": [
    "#text preporcessing \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel(\"sample_customer_database_5000_singapore.xlsx\")\n",
    "\n",
    "# Clean Notes (for all methods)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "df['Cleaned_Notes'] = df['Notes'].apply(preprocess_text)\n",
    "\n",
    "# Save for reuse\n",
    "categorical_cols = ['Location', 'Gender', 'Loyalty Tier']\n",
    "\n",
    "#Preview\n",
    "print(df[['Notes', 'Cleaned_Notes']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e76e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Setup\n",
    "categorical_cols = ['Location', 'Gender', 'Loyalty Tier', 'Join_Year', 'Join_Month', 'Join_Quarter', 'Is_New']\n",
    "numerical_cols = ['Days_Since_Joined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1c7270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model 1 (Text):\n",
      "Silhouette: 0.49039653\n",
      "Calinski-Harabasz: 17360.67001811075\n",
      "Davies-Bouldin: 0.500203221099842\n",
      "✅ Model 1 (Cat):\n",
      "Silhouette: 0.149256281535586\n",
      "Calinski-Harabasz: 418.8640435916867\n",
      "Davies-Bouldin: 2.2547869981359354\n"
     ]
    }
   ],
   "source": [
    " #Model 1: Spectral Clustering (Text + Categorical)\n",
    "# --- Text Embedding using Word2Vec ---\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Tokens'] = df['Cleaned_Notes'].apply(lambda x: [t for t in word_tokenize(str(x)) if t.lower() not in stop_words])\n",
    "w2v_model = Word2Vec(df['Tokens'], vector_size=150, window=5, min_count=1, workers=4)\n",
    "\n",
    "def average_vector(tokens, model, size=150):\n",
    "    valid = [t for t in tokens if t in model.wv]\n",
    "    return np.mean([model.wv[t] for t in valid], axis=0) if valid else np.zeros(size)\n",
    "\n",
    "df['Text_Embeddings'] = df['Tokens'].apply(lambda x: average_vector(x, w2v_model))\n",
    "X_text = np.vstack(df['Text_Embeddings'])\n",
    "X_umap = umap.UMAP(n_neighbors=30, min_dist=0.1, n_components=10).fit_transform(X_text)\n",
    "\n",
    "df['Spectral_Text_Label'] = SpectralClustering(n_clusters=7, affinity='nearest_neighbors').fit_predict(X_umap)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"✅ Model 1 (Text):\")\n",
    "print(\"Silhouette:\", silhouette_score(X_umap, df['Spectral_Text_Label']))\n",
    "print(\"Calinski-Harabasz:\", calinski_harabasz_score(X_umap, df['Spectral_Text_Label']))\n",
    "print(\"Davies-Bouldin:\", davies_bouldin_score(X_umap, df['Spectral_Text_Label']))\n",
    "\n",
    "# --- Categorical Spectral ---\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "cat_encoded = encoder.fit_transform(df[categorical_cols])\n",
    "affinity_matrix = rbf_kernel(cat_encoded, gamma=0.5)\n",
    "df['Spectral_Cat_Label'] = SpectralClustering(n_clusters=5, affinity='precomputed').fit_predict(affinity_matrix)\n",
    "\n",
    "print(\"✅ Model 1 (Cat):\")\n",
    "print(\"Silhouette:\", silhouette_score(cat_encoded, df['Spectral_Cat_Label']))\n",
    "print(\"Calinski-Harabasz:\", calinski_harabasz_score(cat_encoded, df['Spectral_Cat_Label']))\n",
    "print(\"Davies-Bouldin:\", davies_bouldin_score(cat_encoded, df['Spectral_Cat_Label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c886e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model 1 (Spectral) - Text:  7\n",
      "📊 Model 1 (Spectral) - Categorical:  5\n"
     ]
    }
   ],
   "source": [
    "print(\"📊 Model 1 (Spectral) - Text: \", len(np.unique(df['Spectral_Text_Label'])))\n",
    "print(\"📊 Model 1 (Spectral) - Categorical: \", len(np.unique(df['Spectral_Cat_Label'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c3eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# --- Helper for CV + Timing ---\n",
    "def evaluate_with_timing(func, n_runs=5):\n",
    "    sil_scores = []\n",
    "    ch_scores = []\n",
    "    db_scores = []\n",
    "    durations = []\n",
    "    for i in range(n_runs):\n",
    "        start = time.time()\n",
    "        sil, ch, db = func()\n",
    "        end = time.time()\n",
    "        sil_scores.append(sil)\n",
    "        ch_scores.append(ch)\n",
    "        db_scores.append(db)\n",
    "        durations.append(end - start)\n",
    "    return (\n",
    "        np.mean(sil_scores),\n",
    "        np.mean(ch_scores),\n",
    "        np.mean(db_scores),\n",
    "        np.mean(durations),\n",
    "        np.std(sil_scores),\n",
    "        np.std(ch_scores),\n",
    "        np.std(db_scores)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1b3157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model 1 - Text (Spectral)\n",
      "• Avg Silhouette Score: 0.4902 (±0.0000)\n",
      "• Avg Calinski-Harabasz Score: 17328.68 (±0.00)\n",
      "• Avg Davies-Bouldin Score: 0.5001 (±0.0000)\n",
      "• Avg Time: 1.58 seconds\n",
      "\n",
      "✅ Model 1 - Categorical (Spectral)\n",
      "• Avg Silhouette Score: 0.1493 (±0.0000)\n",
      "• Avg Calinski-Harabasz Score: 418.86 (±0.00)\n",
      "• Avg Davies-Bouldin Score: 2.2548 (±0.0000)\n",
      "• Avg Time: 3.70 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- Model 1: Spectral Clustering (Text) ---\n",
    "def run_model1_spectral_text():\n",
    "    labels = SpectralClustering(n_clusters=7, affinity='nearest_neighbors', random_state=42).fit_predict(X_umap)\n",
    "    sil = silhouette_score(X_umap, labels)\n",
    "    ch = calinski_harabasz_score(X_umap, labels)\n",
    "    db = davies_bouldin_score(X_umap, labels)\n",
    "    return sil, ch, db\n",
    "\n",
    "# --- Model 1: Spectral Clustering (Categorical) ---\n",
    "def run_model1_spectral_cat():\n",
    "    labels = SpectralClustering(n_clusters=5, affinity='precomputed').fit_predict(affinity_matrix)\n",
    "    sil = silhouette_score(cat_encoded, labels)\n",
    "    ch = calinski_harabasz_score(cat_encoded, labels)\n",
    "    db = davies_bouldin_score(cat_encoded, labels)\n",
    "    return sil, ch, db\n",
    "\n",
    "# Run CV for both\n",
    "text_sil, text_ch, text_db, text_time, text_sil_std, text_ch_std, text_db_std = evaluate_with_timing(run_model1_spectral_text, n_runs=5)\n",
    "cat_sil, cat_ch, cat_db, cat_time, cat_sil_std, cat_ch_std, cat_db_std = evaluate_with_timing(run_model1_spectral_cat, n_runs=5)\n",
    "\n",
    "# Print results\n",
    "print(\"✅ Model 1 - Text (Spectral)\")\n",
    "print(f\"• Avg Silhouette Score: {text_sil:.4f} (±{text_sil_std:.4f})\")\n",
    "print(f\"• Avg Calinski-Harabasz Score: {text_ch:.2f} (±{text_ch_std:.2f})\")\n",
    "print(f\"• Avg Davies-Bouldin Score: {text_db:.4f} (±{text_db_std:.4f})\")\n",
    "print(f\"• Avg Time: {text_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n✅ Model 1 - Categorical (Spectral)\")\n",
    "print(f\"• Avg Silhouette Score: {cat_sil:.4f} (±{cat_sil_std:.4f})\")\n",
    "print(f\"• Avg Calinski-Harabasz Score: {cat_ch:.2f} (±{cat_ch_std:.2f})\")\n",
    "print(f\"• Avg Davies-Bouldin Score: {cat_db:.4f} (±{cat_db_std:.4f})\")\n",
    "print(f\"• Avg Time: {cat_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8be45e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sil1, text_ch1, text_db1, text_time1, text_sil_std1, text_ch_std1, text_db_std1 = evaluate_with_timing(run_model1_spectral_text, n_runs=5)\n",
    "cat_sil1, cat_ch1, cat_db1, cat_time1, cat_sil_std1, cat_ch_std1, cat_db_std1 = evaluate_with_timing(run_model1_spectral_cat, n_runs=5)\n",
    "\n",
    "text_results1 = (text_sil1, text_ch1, text_db1, text_time1)\n",
    "cat_results1 = (cat_sil1, cat_ch1, cat_db1, cat_time1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae9f773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model 2 (Text - HDBSCAN):\n",
      "• DBCV Score: -0.6268\n",
      "• Calinski-Harabasz Score: 61.28\n",
      "• Davies-Bouldin Score: 0.5939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model 2 (Cat - HDBSCAN):\n",
      "• DBCV Score: 0.9174\n",
      "• Calinski-Harabasz Score: 2200417.56\n",
      "• Davies-Bouldin Score: 0.1252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "from hdbscan.validity import validity_index\n",
    "import hdbscan\n",
    "import umap\n",
    "import numpy as np\n",
    "\n",
    "# ----------- TEXT CLUSTERING ----------- #\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['Cleaned_Notes'].astype(str))\n",
    "X_embed_text = umap.UMAP(n_neighbors=15, min_dist=0.1).fit_transform(X_tfidf)\n",
    "\n",
    "text_clusterer = hdbscan.HDBSCAN(min_cluster_size=10)\n",
    "text_labels = text_clusterer.fit_predict(X_embed_text)\n",
    "df['HDBSCAN_Text_Label'] = text_labels\n",
    "\n",
    "# METRICS (TEXT)\n",
    "mask_text = text_labels != -1\n",
    "if len(np.unique(text_labels[mask_text])) > 1:\n",
    "    X_embed_text_64 = X_embed_text.astype(np.float64)\n",
    "    dbcv_text = validity_index(X_embed_text_64, text_labels)\n",
    "    ch_text = calinski_harabasz_score(X_embed_text[mask_text], text_labels[mask_text])\n",
    "    db_text = davies_bouldin_score(X_embed_text[mask_text], text_labels[mask_text])\n",
    "    print(\"✅ Model 2 (Text - HDBSCAN):\")\n",
    "    print(f\"• DBCV Score: {dbcv_text:.4f}\")\n",
    "    print(f\"• Calinski-Harabasz Score: {ch_text:.2f}\")\n",
    "    print(f\"• Davies-Bouldin Score: {db_text:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ Not enough valid clusters in HDBSCAN Text for metrics.\")\n",
    "\n",
    "\n",
    "# ----------- CATEGORICAL CLUSTERING ----------- #\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "cat_encoded = encoder.fit_transform(df[categorical_cols])\n",
    "cat_embed = umap.UMAP(n_neighbors=15, min_dist=0.1).fit_transform(cat_encoded)\n",
    "\n",
    "cat_clusterer = hdbscan.HDBSCAN(min_cluster_size=10)\n",
    "cat_labels = cat_clusterer.fit_predict(cat_embed)\n",
    "df['HDBSCAN_Cat_Label'] = cat_labels\n",
    "\n",
    "# METRICS (CATEGORICAL)\n",
    "mask_cat = cat_labels != -1\n",
    "if len(np.unique(cat_labels[mask_cat])) > 1:\n",
    "    cat_embed_64 = cat_embed.astype(np.float64)\n",
    "    dbcv_cat = validity_index(cat_embed_64, cat_labels)\n",
    "    ch_cat = calinski_harabasz_score(cat_embed[mask_cat], cat_labels[mask_cat])\n",
    "    db_cat = davies_bouldin_score(cat_embed[mask_cat], cat_labels[mask_cat])\n",
    "    print(\"✅ Model 2 (Cat - HDBSCAN):\")\n",
    "    print(f\"• DBCV Score: {dbcv_cat:.4f}\")\n",
    "    print(f\"• Calinski-Harabasz Score: {ch_cat:.2f}\")\n",
    "    print(f\"• Davies-Bouldin Score: {db_cat:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ Not enough valid clusters in HDBSCAN Cat for metrics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "169ac75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model 2 (HDBSCAN) - Text:  5\n",
      "📊 Model 2 (HDBSCAN) - Categorical:  210\n"
     ]
    }
   ],
   "source": [
    "# Text\n",
    "n_clusters_text = len(np.unique(df['HDBSCAN_Text_Label'])) - (1 if -1 in df['HDBSCAN_Text_Label'] else 0)\n",
    "print(\"📊 Model 2 (HDBSCAN) - Text: \", n_clusters_text)\n",
    "\n",
    "# Categorical\n",
    "n_clusters_cat = len(np.unique(df['HDBSCAN_Cat_Label'])) - (1 if -1 in df['HDBSCAN_Cat_Label'] else 0)\n",
    "print(\"📊 Model 2 (HDBSCAN) - Categorical: \", n_clusters_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4156f4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model 2 - Text (HDBSCAN)\n",
      "• Avg DBCV Score: -0.5680 (±0.0000)\n",
      "• Avg Calinski-Harabasz Score: 58.97 (±0.00)\n",
      "• Avg Davies-Bouldin Score: 0.6259 (±0.0000)\n",
      "• Avg Time: 11.08 seconds\n",
      "\n",
      "✅ Model 2 - Categorical (HDBSCAN)\n",
      "• Avg DBCV Score: 0.9071 (±0.0000)\n",
      "• Avg Calinski-Harabasz Score: 1698410.62 (±0.00)\n",
      "• Avg Davies-Bouldin Score: 0.1189 (±0.0000)\n",
      "• Avg Time: 9.59 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- Model 2: HDBSCAN (Text) ---\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['Cleaned_Notes'].astype(str))\n",
    "X_embed_text = umap.UMAP(n_neighbors=15, min_dist=0.1).fit_transform(X_tfidf)\n",
    "\n",
    "def run_model2_hdbscan_text():\n",
    "    labels = hdbscan.HDBSCAN(min_cluster_size=10).fit_predict(X_embed_text)\n",
    "    mask = labels != -1\n",
    "    if len(np.unique(labels[mask])) > 1:\n",
    "        dbcv = validity_index(X_embed_text.astype(np.float64), labels)\n",
    "        ch = calinski_harabasz_score(X_embed_text[mask], labels[mask])\n",
    "        db = davies_bouldin_score(X_embed_text[mask], labels[mask])\n",
    "        return dbcv, ch, db\n",
    "    return 0, 0, 0\n",
    "\n",
    "# --- Model 2: HDBSCAN (Categorical) ---\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "cat_encoded = encoder.fit_transform(df[['Location', 'Gender', 'Loyalty Tier', 'Join_Year', 'Join_Month', 'Join_Quarter', 'Is_New']])\n",
    "cat_embed = umap.UMAP(n_neighbors=15, min_dist=0.1).fit_transform(cat_encoded)\n",
    "\n",
    "def run_model2_hdbscan_cat():\n",
    "    labels = hdbscan.HDBSCAN(min_cluster_size=10).fit_predict(cat_embed)\n",
    "    mask = labels != -1\n",
    "    if len(np.unique(labels[mask])) > 1:\n",
    "        dbcv = validity_index(cat_embed.astype(np.float64), labels)\n",
    "        ch = calinski_harabasz_score(cat_embed[mask], labels[mask])\n",
    "        db = davies_bouldin_score(cat_embed[mask], labels[mask])\n",
    "        return dbcv, ch, db\n",
    "    return 0, 0, 0\n",
    "\n",
    "# Run CV for Model 2\n",
    "text_sil2, text_ch2, text_db2, text_time2, text_sil_std2, text_ch_std2, text_db_std2 = evaluate_with_timing(run_model2_hdbscan_text, n_runs=5)\n",
    "cat_sil2, cat_ch2, cat_db2, cat_time2, cat_sil_std2, cat_ch_std2, cat_db_std2 = evaluate_with_timing(run_model2_hdbscan_cat, n_runs=5)\n",
    "\n",
    "print(\"\\n✅ Model 2 - Text (HDBSCAN)\")\n",
    "print(f\"• Avg DBCV Score: {text_sil2:.4f} (±{text_sil_std2:.4f})\")\n",
    "print(f\"• Avg Calinski-Harabasz Score: {text_ch2:.2f} (±{text_ch_std2:.2f})\")\n",
    "print(f\"• Avg Davies-Bouldin Score: {text_db2:.4f} (±{text_db_std2:.4f})\")\n",
    "print(f\"• Avg Time: {text_time2:.2f} seconds\")\n",
    "\n",
    "print(\"\\n✅ Model 2 - Categorical (HDBSCAN)\")\n",
    "print(f\"• Avg DBCV Score: {cat_sil2:.4f} (±{cat_sil_std2:.4f})\")\n",
    "print(f\"• Avg Calinski-Harabasz Score: {cat_ch2:.2f} (±{cat_ch_std2:.2f})\")\n",
    "print(f\"• Avg Davies-Bouldin Score: {cat_db2:.4f} (±{cat_db_std2:.4f})\")\n",
    "print(f\"• Avg Time: {cat_time2:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1037f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 - Text: 4 clusters\n",
      "Model 2 - Categorical: 209 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\ML models\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get final labels for cluster count\n",
    "hdbscan_labels_text = hdbscan.HDBSCAN(min_cluster_size=10).fit_predict(X_embed_text)\n",
    "hdbscan_labels_cat = hdbscan.HDBSCAN(min_cluster_size=10).fit_predict(cat_embed)\n",
    "\n",
    "# Count number of clusters (excluding noise label -1)\n",
    "n_clusters_text2 = len(set(hdbscan_labels_text)) - (1 if -1 in hdbscan_labels_text else 0)\n",
    "n_clusters_cat2 = len(set(hdbscan_labels_cat)) - (1 if -1 in hdbscan_labels_cat else 0)\n",
    "\n",
    "print(f\"Model 2 - Text: {n_clusters_text2} clusters\")\n",
    "print(f\"Model 2 - Categorical: {n_clusters_cat2} clusters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbd57169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure this runs before Model 3\n",
    "df['Date Joined'] = pd.to_datetime(df['Date Joined'])\n",
    "df['Days_Since_Joined'] = (pd.Timestamp.today() - df['Date Joined']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0352735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model 3 (Hybrid):\n",
      "Silhouette: 0.12741052792524693\n",
      "Calinski-Harabasz: 420.8231533774878\n",
      "Davies-Bouldin: 2.3096629716962687\n"
     ]
    }
   ],
   "source": [
    "#model 3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- BERT for text ---\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeds = bert.encode(df['Cleaned_Notes'].astype(str).tolist())\n",
    "sim_text = cosine_similarity(bert_embeds)\n",
    "\n",
    "# --- Jaccard for categorical ---\n",
    "inter = np.dot(cat_encoded, cat_encoded.T)\n",
    "rowsum = cat_encoded.sum(axis=1)\n",
    "union = rowsum[:, None] + rowsum - inter\n",
    "sim_cat = inter / np.maximum(union, 1e-10)\n",
    "\n",
    "# --- Combine Similarity Matrices ---\n",
    "alpha = 0.5\n",
    "S = alpha * sim_text + (1 - alpha) * sim_cat\n",
    "dist = 1 - S\n",
    "\n",
    "# --- Agglomerative Clustering ---\n",
    "agglo = AgglomerativeClustering(n_clusters=5, metric='precomputed', linkage='average')\n",
    "df['Hybrid_Agglo_Label'] = agglo.fit_predict(dist)\n",
    "\n",
    "# --- Evaluation on Combined Encoded Features ---\n",
    "scaled_num = StandardScaler().fit_transform(df[numerical_cols])\n",
    "combined = np.hstack([cat_encoded, scaled_num])\n",
    "print(\"✅ Model 3 (Hybrid):\")\n",
    "print(\"Silhouette:\", silhouette_score(combined, df['Hybrid_Agglo_Label']))\n",
    "print(\"Calinski-Harabasz:\", calinski_harabasz_score(combined, df['Hybrid_Agglo_Label']))\n",
    "print(\"Davies-Bouldin:\", davies_bouldin_score(combined, df['Hybrid_Agglo_Label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "768c3ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model 3 (Hybrid Agglomerative):  5\n"
     ]
    }
   ],
   "source": [
    "print(\"📊 Model 3 (Hybrid Agglomerative): \", len(np.unique(df['Hybrid_Agglo_Label'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4493543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model 3 - Hybrid (Agglomerative)\n",
      "• Avg Silhouette Score: 0.1274 (±0.0000)\n",
      "• Avg Calinski-Harabasz Score: 420.82 (±0.00)\n",
      "• Avg Davies-Bouldin Score: 2.3097 (±0.0000)\n",
      "• Avg Time: 1.40 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Function to run model\n",
    "def run_model3_agglo():\n",
    "    labels = AgglomerativeClustering(n_clusters=5, metric='precomputed', linkage='average').fit_predict(dist)\n",
    "    scaled_num = StandardScaler().fit_transform(df[numerical_cols])\n",
    "    combined = np.hstack([cat_encoded, scaled_num])\n",
    "    sil = silhouette_score(combined, labels)\n",
    "    ch = calinski_harabasz_score(combined, labels)\n",
    "    db = davies_bouldin_score(combined, labels)\n",
    "    return sil, ch, db\n",
    "\n",
    "# CV Evaluation function\n",
    "def evaluate_with_timing(func, n_runs=5):\n",
    "    sils, chs, dbs, times = [], [], [], []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        sil, ch, db = func()\n",
    "        end = time.time()\n",
    "        sils.append(sil)\n",
    "        chs.append(ch)\n",
    "        dbs.append(db)\n",
    "        times.append(end - start)\n",
    "    return (\n",
    "        np.mean(sils), np.mean(chs), np.mean(dbs),\n",
    "        np.mean(times),\n",
    "        np.std(sils), np.std(chs), np.std(dbs)\n",
    "    )\n",
    "\n",
    "# Run CV\n",
    "sil3, ch3, db3, time3, sil3_std, ch3_std, db3_std = evaluate_with_timing(run_model3_agglo, n_runs=5)\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n✅ Model 3 - Hybrid (Agglomerative)\")\n",
    "print(f\"• Avg Silhouette Score: {sil3:.4f} (±{sil3_std:.4f})\")\n",
    "print(f\"• Avg Calinski-Harabasz Score: {ch3:.2f} (±{ch3_std:.2f})\")\n",
    "print(f\"• Avg Davies-Bouldin Score: {db3:.4f} (±{db3_std:.4f})\")\n",
    "print(f\"• Avg Time: {time3:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d2c1f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\users\\lenovo\\downloads\\year 3 major project\\ml models\\.venv\\lib\\site-packages (0.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20613cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Comparison Table:\n",
      "\n",
      "+---+----------------------------------+-----------------+-------------------+--------------------+------------------+------------+----------+\n",
      "|   |              Model               | No. of Clusters | Silhouette (±SD)  |   CH Score (±SD)   |  DB Score (±SD)  | DBCV Score | Time (s) |\n",
      "+---+----------------------------------+-----------------+-------------------+--------------------+------------------+------------+----------+\n",
      "| 0 |    Model 1 - Text (Spectral)     |        7        | 0.4902 (±0.0000)  |  17328.68 (±0.00)  | 0.5001 (±0.0000) |     -      |   1.58   |\n",
      "| 1 |     Model 1 - Cat (Spectral)     |        5        | 0.1493 (±0.0000)  |   418.86 (±0.00)   | 2.2548 (±0.0000) |     -      |   3.70   |\n",
      "| 2 |     Model 2 - Text (HDBSCAN)     |        4        | -0.5680 (±0.0000) |   61.28 (±0.00)    | 0.5939 (±0.0000) |  -0.6268   |  11.08   |\n",
      "| 3 |     Model 2 - Cat (HDBSCAN)      |       209       | 0.9071 (±0.0000)  | 2200417.56 (±0.00) | 0.1252 (±0.0000) |   0.9174   |   9.59   |\n",
      "| 4 | Model 3 - Hybrid (Agglomerative) |        5        | 0.1274 (±0.0000)  |   420.82 (±0.00)   | 2.3097 (±0.0000) |     -      |   1.40   |\n",
      "+---+----------------------------------+-----------------+-------------------+--------------------+------------------+------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from hdbscan.validity import validity_index\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Step 1: Model results using your variable names\n",
    "models_data = [\n",
    "    (\"Model 1 - Text (Spectral)\", 7, text_sil, text_sil_std, text_ch, text_ch_std, text_db, text_db_std, text_time, \"-\"),\n",
    "    (\"Model 1 - Cat (Spectral)\", 5, cat_sil, cat_sil_std, cat_ch, cat_ch_std, cat_db, cat_db_std, cat_time, \"-\"),\n",
    "    (\"Model 2 - Text (HDBSCAN)\", len(np.unique(text_labels[text_labels != -1])), text_sil2, text_sil_std2, ch_text, 0, db_text, 0, text_time2, dbcv_text),\n",
    "    (\"Model 2 - Cat (HDBSCAN)\", len(np.unique(cat_labels[cat_labels != -1])), cat_sil2, cat_sil_std2, ch_cat, 0, db_cat, 0, cat_time2, dbcv_cat),\n",
    "    (\"Model 3 - Hybrid (Agglomerative)\", 5, sil3, sil3_std, ch3, ch3_std, db3, db3_std, time3, \"-\")\n",
    "]\n",
    "\n",
    "# Step 2: Create DataFrame\n",
    "df_models = pd.DataFrame(models_data, columns=[\n",
    "    \"Model\", \"No. of Clusters\",\n",
    "    \"Silhouette\", \"Sil Std\",\n",
    "    \"Calinski-Harabasz\", \"CH Std\",\n",
    "    \"Davies-Bouldin\", \"DB Std\",\n",
    "    \"Time (s)\", \"DBCV Score\"\n",
    "])\n",
    "\n",
    "# Step 3: Format for pretty display\n",
    "df_models[\"Silhouette (±SD)\"] = df_models.apply(lambda x: f\"{x['Silhouette']:.4f} (±{x['Sil Std']:.4f})\", axis=1)\n",
    "df_models[\"CH Score (±SD)\"] = df_models.apply(lambda x: f\"{x['Calinski-Harabasz']:.2f} (±{x['CH Std']:.2f})\", axis=1)\n",
    "df_models[\"DB Score (±SD)\"] = df_models.apply(lambda x: f\"{x['Davies-Bouldin']:.4f} (±{x['DB Std']:.4f})\", axis=1)\n",
    "df_models[\"Time (s)\"] = df_models[\"Time (s)\"].apply(lambda x: f\"{x:.2f}\" if isinstance(x, (int, float)) else x)\n",
    "df_models[\"DBCV Score\"] = df_models[\"DBCV Score\"].apply(lambda x: f\"{x:.4f}\" if isinstance(x, float) else x)\n",
    "\n",
    "# Step 4: Final formatted display\n",
    "final_table = df_models[[  \n",
    "    \"Model\", \"No. of Clusters\",  \n",
    "    \"Silhouette (±SD)\", \"CH Score (±SD)\", \"DB Score (±SD)\",  \n",
    "    \"DBCV Score\", \"Time (s)\"  \n",
    "]]\n",
    "\n",
    "print(\"📊 Model Comparison Table:\\n\")\n",
    "print(tabulate(final_table, headers='keys', tablefmt='pretty'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e44ec0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Comparison Table:\n",
      "\n",
      "+---+----------------------------------+-----------------+-------------------+--------------------+------------------+----------+\n",
      "|   |              Model               | No. of Clusters | Silhouette (±SD)  |   CH Score (±SD)   |  DB Score (±SD)  | Time (s) |\n",
      "+---+----------------------------------+-----------------+-------------------+--------------------+------------------+----------+\n",
      "| 0 |    Model 1 - Text (Spectral)     |        7        | 0.4902 (±0.0000)  |  17328.68 (±0.00)  | 0.5001 (±0.0000) |   1.58   |\n",
      "| 1 |     Model 1 - Cat (Spectral)     |        5        | 0.1493 (±0.0000)  |   418.86 (±0.00)   | 2.2548 (±0.0000) |   3.70   |\n",
      "| 2 |     Model 2 - Text (HDBSCAN)     |        4        | -0.5680 (±0.0000) |   58.97 (±0.00)    | 0.6259 (±0.0000) |  11.08   |\n",
      "| 3 |     Model 2 - Cat (HDBSCAN)      |       209       | 0.9071 (±0.0000)  | 1698410.62 (±0.00) | 0.1189 (±0.0000) |   9.59   |\n",
      "| 4 | Model 3 - Hybrid (Agglomerative) |        5        | 0.1274 (±0.0000)  |   420.82 (±0.00)   | 2.3097 (±0.0000) |   1.40   |\n",
      "+---+----------------------------------+-----------------+-------------------+--------------------+------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Model results: (Model Name, No. of Clusters, Silhouette, Sil Std, CH, CH Std, DB, DB Std, Time)\n",
    "models_data = [\n",
    "    (\"Model 1 - Text (Spectral)\", 7, text_sil, text_sil_std, text_ch, text_ch_std, text_db, text_db_std, text_time),\n",
    "    (\"Model 1 - Cat (Spectral)\", 5, cat_sil, cat_sil_std, cat_ch, cat_ch_std, cat_db, cat_db_std, cat_time),\n",
    "    (\"Model 2 - Text (HDBSCAN)\", n_clusters_text2, text_sil2, text_sil_std2, text_ch2, text_ch_std2, text_db2, text_db_std2, text_time2),\n",
    "    (\"Model 2 - Cat (HDBSCAN)\", n_clusters_cat2, cat_sil2, cat_sil_std2, cat_ch2, cat_ch_std2, cat_db2, cat_db_std2, cat_time2),\n",
    "    (\"Model 3 - Hybrid (Agglomerative)\", 5, sil3, sil3_std, ch3, ch3_std, db3, db3_std, time3)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_models = pd.DataFrame(models_data, columns=[\n",
    "    \"Model\", \"No. of Clusters\",\n",
    "    \"Silhouette\", \"Sil Std\",\n",
    "    \"Calinski-Harabasz\", \"CH Std\",\n",
    "    \"Davies-Bouldin\", \"DB Std\",\n",
    "    \"Time (s)\"\n",
    "])\n",
    "\n",
    "# Format for pretty display\n",
    "df_models[\"Silhouette (±SD)\"] = df_models.apply(lambda x: f\"{x['Silhouette']:.4f} (±{x['Sil Std']:.4f})\", axis=1)\n",
    "df_models[\"CH Score (±SD)\"] = df_models.apply(lambda x: f\"{x['Calinski-Harabasz']:.2f} (±{x['CH Std']:.2f})\", axis=1)\n",
    "df_models[\"DB Score (±SD)\"] = df_models.apply(lambda x: f\"{x['Davies-Bouldin']:.4f} (±{x['DB Std']:.4f})\", axis=1)\n",
    "df_models[\"Time (s)\"] = df_models[\"Time (s)\"].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "# Final formatted display\n",
    "final_table = df_models[[\n",
    "    \"Model\", \"No. of Clusters\",\n",
    "    \"Silhouette (±SD)\", \"CH Score (±SD)\", \"DB Score (±SD)\", \"Time (s)\"\n",
    "]]\n",
    "\n",
    "# Display with tabulate\n",
    "print(\"📊 Model Comparison Table:\\n\")\n",
    "print(tabulate(final_table, headers='keys', tablefmt='pretty'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a1415f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
