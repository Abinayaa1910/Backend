{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "026395ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel('sample_customer_database_5000_singapore.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621ab285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Customer ID        Full Name                Email Address  Phone Number  \\\n",
      "0       C0001     Norma Fisher          ysullivan@yahoo.com      82421948   \n",
      "1       C0002      Levi Durham            qgrimes@gmail.com      97535139   \n",
      "2       C0003   Kimberly Olsen  sean96@johnston-roberts.com      71122018   \n",
      "3       C0004   Matthew Davies    nguyendarrell@hotmail.com      41352560   \n",
      "4       C0005  Angela Martinez    myersmitchell@johnson.com        869141   \n",
      "\n",
      "  Date Joined     Location  Gender Loyalty Tier  \\\n",
      "0  2023-08-11     Tampines  Female     Platinum   \n",
      "1  2022-11-24      Geylang  Female     Platinum   \n",
      "2  2023-06-19     Tampines  Female     Platinum   \n",
      "3  2025-04-04   Ang Mo Kio    Male       Silver   \n",
      "4  2025-01-15  Bukit Batok  Female     Platinum   \n",
      "\n",
      "                                               Notes  \n",
      "0                        Together range line beyond.  \n",
      "1  Language ball floor meet usually board necessary.  \n",
      "2                 Support time operation wear often.  \n",
      "3                                  Stage plant view.  \n",
      "4          Job article level others record hospital.  \n"
     ]
    }
   ],
   "source": [
    "# First look at the data\n",
    "print(df.head())  # First 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ae35ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "\n",
    "print(\"NumPy version:\", numpy.__version__)\n",
    "print(\"Pandas version:\", pandas.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ccb804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (5000, 9)\n"
     ]
    }
   ],
   "source": [
    "# Shape of the dataset\n",
    "print(\"Shape of dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbeab334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Customer ID    5000 non-null   object        \n",
      " 1   Full Name      5000 non-null   object        \n",
      " 2   Email Address  5000 non-null   object        \n",
      " 3   Phone Number   5000 non-null   int64         \n",
      " 4   Date Joined    5000 non-null   datetime64[ns]\n",
      " 5   Location       5000 non-null   object        \n",
      " 6   Gender         5000 non-null   object        \n",
      " 7   Loyalty Tier   5000 non-null   object        \n",
      " 8   Notes          5000 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(7)\n",
      "memory usage: 351.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Columns and Data types\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2290bca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " Customer ID      0\n",
      "Full Name        0\n",
      "Email Address    0\n",
      "Phone Number     0\n",
      "Date Joined      0\n",
      "Location         0\n",
      "Gender           0\n",
      "Loyalty Tier     0\n",
      "Notes            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d16c175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"Number of duplicate rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b711366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values per column:\n",
      " Customer ID      5000\n",
      "Full Name        4835\n",
      "Email Address    4983\n",
      "Phone Number     4998\n",
      "Date Joined      1084\n",
      "Location           27\n",
      "Gender              2\n",
      "Loyalty Tier        3\n",
      "Notes            5000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Unique values per column\n",
    "print(\"Unique values per column:\\n\", df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9feda75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Notes  \\\n",
      "0                        Together range line beyond.   \n",
      "1  Language ball floor meet usually board necessary.   \n",
      "2                 Support time operation wear often.   \n",
      "3                                  Stage plant view.   \n",
      "4          Job article level others record hospital.   \n",
      "\n",
      "                                      Cleaned_Notes  \n",
      "0                        together range line beyond  \n",
      "1  language ball floor meet usually board necessary  \n",
      "2                 support time operation wear often  \n",
      "3                                  stage plant view  \n",
      "4          job article level others record hospital  \n"
     ]
    }
   ],
   "source": [
    "#text preporcessing \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel(\"sample_customer_database_5000_singapore.xlsx\")\n",
    "\n",
    "# Clean Notes (for all methods)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "df['Cleaned_Notes'] = df['Notes'].apply(preprocess_text)\n",
    "\n",
    "# Save for reuse\n",
    "categorical_cols = ['Location', 'Gender', 'Loyalty Tier']\n",
    "\n",
    "#Preview\n",
    "print(df[['Notes', 'Cleaned_Notes']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa31bbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\marketing-portal\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\marketing-portal\\.venv\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model 1 - Spectral + UMAP (Improved)\n",
      "Silhouette Score: 0.48380023\n"
     ]
    }
   ],
   "source": [
    "#model 1:Model 1: Spectral Clustering with Word2Vec for Text + One-Hot for Categorical (Separate)\n",
    "# #text clustering\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import umap\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Token cleaning\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Tokens'] = df['Cleaned_Notes'].apply(lambda x: [t for t in word_tokenize(x) if t not in stop_words])\n",
    "\n",
    "# Step 2: Word2Vec (baseline)\n",
    "w2v_model = Word2Vec(df['Tokens'], vector_size=150, window=5, min_count=1, workers=4)\n",
    "\n",
    "def average_vector(tokens, model, size=150):\n",
    "    valid_tokens = [token for token in tokens if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(size)\n",
    "    return np.mean([model.wv[token] for token in valid_tokens], axis=0)\n",
    "\n",
    "df['Text_Embeddings'] = df['Tokens'].apply(lambda x: average_vector(x, w2v_model))\n",
    "X_text = np.vstack(df['Text_Embeddings'].values)\n",
    "\n",
    "# Step 3: UMAP\n",
    "X_umap = umap.UMAP(n_neighbors=30, min_dist=0.1, n_components=10, random_state=42).fit_transform(X_text)\n",
    "\n",
    "# Step 4: Spectral Clustering\n",
    "spectral = SpectralClustering(n_clusters=7, affinity='nearest_neighbors', random_state=42)\n",
    "df['Text_Spectral_Label'] = spectral.fit_predict(X_umap)\n",
    "\n",
    "# Step 5: Evaluate\n",
    "score = silhouette_score(X_umap, df['Text_Spectral_Label'])\n",
    "print(\"✅ Model 1 - Spectral + UMAP (Improved)\")\n",
    "print(\"Silhouette Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d68558f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model 1 - Spectral + UMAP (Improved)\n",
      "Silhouette Score: 0.2794308491074161\n"
     ]
    }
   ],
   "source": [
    "#improvised categorical clustering\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "cat_encoded = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "affinity_matrix = rbf_kernel(cat_encoded, gamma=0.5)\n",
    "df['Cat_Spectral_Label'] = SpectralClustering(n_clusters=5, affinity='precomputed').fit_predict(affinity_matrix)\n",
    "\n",
    "# Step 5: Evaluate\n",
    "score = silhouette_score(cat_encoded, df['Cat_Spectral_Label'])\n",
    "print(\"✅ Model 1 - Spectral + UMAP (Improved)\")\n",
    "print(\"Silhouette Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485ecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Spectral Clustering (Text) — Cluster Counts\n",
      "Text_Spectral_Label\n",
      "1    2174\n",
      "4    1526\n",
      "0     910\n",
      "2     254\n",
      "3     136\n",
      "Name: count, dtype: int64\n",
      "\n",
      "📝 Sample Notes per Text Cluster\n",
      "\n",
      "--- Cluster 0 ---\n",
      "                                  stage plant view\n",
      "response purpose character would partner hit an...\n",
      "                        know series lay smile away\n",
      "\n",
      "--- Cluster 1 ---\n",
      "              together range line beyond\n",
      "job article level others record hospital\n",
      "                           part cup read\n",
      "\n",
      "--- Cluster 2 ---\n",
      "        movie end discussion budget situation run\n",
      "                                  time firm water\n",
      "recently prepare scene house central baby picture\n",
      "\n",
      "--- Cluster 3 ---\n",
      "suffer without rather\n",
      "     pm election case\n",
      " rather spend similar\n",
      "\n",
      "--- Cluster 4 ---\n",
      "language ball floor meet usually board necessary\n",
      "               support time operation wear often\n",
      "                            animal exactly drive\n",
      "\n",
      "📊 Spectral Clustering (Categorical) — Cluster Counts\n",
      "Cat_Spectral_Label\n",
      "4    2809\n",
      "1    1995\n",
      "3      80\n",
      "2      60\n",
      "0      56\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🏷️ Sample Records per Categorical Cluster\n",
      "\n",
      "--- Cluster 0 ---\n",
      " Location Gender Loyalty Tier\n",
      " Tampines Female         Gold\n",
      " Tampines Female         Gold\n",
      "Woodlands Female     Platinum\n",
      "\n",
      "--- Cluster 1 ---\n",
      " Location Gender Loyalty Tier\n",
      "    Bedok Female       Silver\n",
      "Woodlands Female       Silver\n",
      "   Yishun   Male     Platinum\n",
      "\n",
      "--- Cluster 2 ---\n",
      "Location Gender Loyalty Tier\n",
      "   Bedok Female         Gold\n",
      "   Bedok Female         Gold\n",
      "   Bedok Female         Gold\n",
      "\n",
      "--- Cluster 3 ---\n",
      "    Location Gender Loyalty Tier\n",
      "Central Area Female       Silver\n",
      "    Clementi Female         Gold\n",
      "      Yishun Female     Platinum\n",
      "\n",
      "--- Cluster 4 ---\n",
      "Location Gender Loyalty Tier\n",
      "Tampines Female     Platinum\n",
      " Geylang Female     Platinum\n",
      "Tampines Female     Platinum\n"
     ]
    }
   ],
   "source": [
    "# Display Spectral Clustering results for TEXT\n",
    "print(\"📊 Spectral Clustering (Text) — Cluster Counts\")\n",
    "print(df['Text_Spectral_Label'].value_counts())\n",
    "\n",
    "print(\"\\n📝 Sample Notes per Text Cluster\")\n",
    "for i in sorted(df['Text_Spectral_Label'].unique()):\n",
    "    print(f\"\\n--- Cluster {i} ---\")\n",
    "    print(df[df['Text_Spectral_Label'] == i]['Cleaned_Notes'].head(3).to_string(index=False))\n",
    "\n",
    "# Display Spectral Clustering results for CATEGORICAL\n",
    "print(\"\\n📊 Spectral Clustering (Categorical) — Cluster Counts\")\n",
    "print(df['Cat_Spectral_Label'].value_counts())\n",
    "\n",
    "print(\"\\n🏷️ Sample Records per Categorical Cluster\")\n",
    "for i in sorted(df['Cat_Spectral_Label'].unique()):\n",
    "    print(f\"\\n--- Cluster {i} ---\")\n",
    "    print(df[df['Cat_Spectral_Label'] == i][['Location', 'Gender', 'Loyalty Tier']].head(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a5f5fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\marketing-portal\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\marketing-portal\\.venv\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\marketing-portal\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\marketing-portal\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\marketing-portal\\.venv\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Cleaned_Notes  HDBSCAN_Text_Label  \\\n",
      "0                        together range line beyond                  -1   \n",
      "1  language ball floor meet usually board necessary                  -1   \n",
      "2                 support time operation wear often                  49   \n",
      "3                                  stage plant view                  66   \n",
      "4          job article level others record hospital                  47   \n",
      "\n",
      "   HDBSCAN_Cat_Label  \n",
      "0                147  \n",
      "1                155  \n",
      "2                147  \n",
      "3                 17  \n",
      "4                137  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\marketing-portal\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\Year 3 Major project\\marketing-portal\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Model 2: HDBSCAN with UMAP Embedding for Text + Cat (Separate)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "# ---------- TEXT CLUSTERING ----------\n",
    "# Step 1: TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Cleaned_Notes'].astype(str))\n",
    "\n",
    "# Step 2: UMAP Dimensionality Reduction\n",
    "text_embed = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42).fit_transform(tfidf_matrix)\n",
    "\n",
    "# Step 3: HDBSCAN Clustering\n",
    "df['HDBSCAN_Text_Label'] = hdbscan.HDBSCAN(min_cluster_size=10).fit_predict(text_embed)\n",
    "\n",
    "# ---------- CATEGORICAL CLUSTERING ----------\n",
    "# Step 1: UMAP on One-Hot Encoded Data\n",
    "cat_embed = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42).fit_transform(cat_encoded)\n",
    "\n",
    "# Step 2: HDBSCAN Clustering\n",
    "df['HDBSCAN_Cat_Label'] = hdbscan.HDBSCAN(min_cluster_size=10).fit_predict(cat_embed)\n",
    "\n",
    "# ---------- Preview ----------\n",
    "print(df[['Cleaned_Notes', 'HDBSCAN_Text_Label', 'HDBSCAN_Cat_Label']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "318f626f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Location  Gender Loyalty Tier  \\\n",
      "0     Tampines  Female     Platinum   \n",
      "1      Geylang  Female     Platinum   \n",
      "2     Tampines  Female     Platinum   \n",
      "3   Ang Mo Kio    Male       Silver   \n",
      "4  Bukit Batok  Female     Platinum   \n",
      "\n",
      "                                      Cleaned_Notes  Hybrid_Agglo_Label  \n",
      "0                        together range line beyond                   0  \n",
      "1  language ball floor meet usually board necessary                   0  \n",
      "2                 support time operation wear often                   0  \n",
      "3                                  stage plant view                   3  \n",
      "4          job article level others record hospital                   0  \n"
     ]
    }
   ],
   "source": [
    "#updated method 3:#model 3 :Agglomerative Clustering with Gower Distance on Combined Features (With BERT)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "# BERT Embeddings\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = bert_model.encode(df['Cleaned_Notes'].astype(str).tolist())\n",
    "\n",
    "# Cosine Similarity from BERT\n",
    "A = cosine_similarity(bert_embeddings)\n",
    "\n",
    "# One-hot encode categorical data\n",
    "categorical_cols = ['Location', 'Gender', 'Loyalty Tier']\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "cat_encoded = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Jaccard Similarity for categorical data\n",
    "intersection = np.dot(cat_encoded, cat_encoded.T)\n",
    "row_sums = cat_encoded.sum(axis=1)\n",
    "union = row_sums[:, None] + row_sums - intersection\n",
    "B = intersection / np.maximum(union, 1e-10)\n",
    "\n",
    "# Combine similarities\n",
    "alpha = 0.5\n",
    "S = alpha * A + (1 - alpha) * B\n",
    "distance_matrix = 1 - S\n",
    "\n",
    "# Agglomerative Clustering\n",
    "agglo = AgglomerativeClustering(n_clusters=5, metric='precomputed', linkage='average')\n",
    "df['Hybrid_Agglo_Label'] = agglo.fit_predict(distance_matrix)\n",
    "\n",
    "# View sample\n",
    "print(df[['Location', 'Gender', 'Loyalty Tier', 'Cleaned_Notes', 'Hybrid_Agglo_Label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e29e8aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 - Text (HDBSCAN)\n",
      "Silhouette: 0.48412868\n",
      "Model 2 - Cat (HDBSCAN)\n",
      "Silhouette: 0.9875996\n"
     ]
    }
   ],
   "source": [
    "# Remove noise points (-1) before calculating metrics\n",
    "text_mask = df['HDBSCAN_Text_Label'] != -1\n",
    "cat_mask = df['HDBSCAN_Cat_Label'] != -1\n",
    "\n",
    "print(\"Model 2 - Text (HDBSCAN)\")\n",
    "print(\"Silhouette:\", silhouette_score(text_embed[text_mask], df.loc[text_mask, 'HDBSCAN_Text_Label']))\n",
    "\n",
    "print(\"Model 2 - Cat (HDBSCAN)\")\n",
    "print(\"Silhouette:\", silhouette_score(cat_embed[cat_mask], df.loc[cat_mask, 'HDBSCAN_Cat_Label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93a8ba36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 - Calinski-Harabasz Score: 622.1816412837292\n",
      "Model 3 - Davies-Bouldin Score: 2.125900885767681\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Reuse combined features (used to create similarity matrix)\n",
    "# This includes BERT embeddings + one-hot categorical encoding\n",
    "combined_features = np.hstack((cat_encoded, bert_embeddings))\n",
    "\n",
    "# Get labels from Model 3\n",
    "labels = df['Hybrid_Agglo_Label']\n",
    "\n",
    "# Calinski-Harabasz Score (higher is better)\n",
    "ch_score = calinski_harabasz_score(combined_features, labels)\n",
    "\n",
    "# Davies-Bouldin Score (lower is better)\n",
    "db_score = davies_bouldin_score(combined_features, labels)\n",
    "\n",
    "print(\"Model 3 - Calinski-Harabasz Score:\", ch_score)\n",
    "print(\"Model 3 - Davies-Bouldin Score:\", db_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
